{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd2dff1-4fcb-4807-8a73-514f8609b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b787b-fe52-4f25-8c0a-29ddad4ff3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cab96-fd26-4edc-a766-db6bdaf93b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b851c-c989-425e-bd08-2b7412969e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "\n",
    "\n",
    "NUM_STATES = env.observation_space.shape[0]\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "\n",
    "print(f'States: {NUM_STATES} all ot them continuius')\n",
    "print(f'theta1 is the angle of the first joint')\n",
    "print(\"theta2 is relative to the angle of the first link\")\n",
    "print('Actions: {}'.format(NUM_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d55a05-df46-4534-ab40-02564299bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shared network so actor and critic study the game simultaniously\n",
    "class A2CNetwork(tf.keras.Model):\n",
    "    def __init__(self, n_actions,):\n",
    "        super(A2CNetwork, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.layer1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.layer2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        \n",
    "        self.v = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.pi = tf.keras.layers.Dense(128, activation='relu')\n",
    "        \n",
    "        self.v_out = tf.keras.layers.Dense(1, activation=None)#value of position can be out of [0,1] interval\n",
    "        self.pi_out = tf.keras.layers.Dense(self.n_actions, activation='softmax')#policy  action\n",
    "\n",
    "    def call(self, state):\n",
    "        value = self.layer1(state)\n",
    "        value = self.layer2(value)\n",
    "        \n",
    "        value = self.v(value)\n",
    "        pi = self.pi(value)\n",
    "\n",
    "        v = self.v_out(value)#value \n",
    "        pi = self.pi_out(pi)#policy\n",
    "\n",
    "        return v, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c807c2-e22c-415f-9f1e-d65168d03dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, lr, gamma, NUM_ACTIONS,NUM_STATES, batch_size, epsilon=0.95, ):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_decay = 0.001\n",
    "        self.epsilon_final = 0.05\n",
    "        self.update_rate = 100\n",
    "        self.step_counter = 0\n",
    "        self.buffer = ReplayBuffer(100000)\n",
    "        self.q_net = Q_Model(lr, NUM_ACTIONS, NUM_STATES,)\n",
    "        self.q_target_net = Q_Model(lr, NUM_ACTIONS, NUM_STATES, )\n",
    "        \n",
    "    \n",
    "    def train_step(self):\n",
    "        \n",
    "        #update target network after n steps\n",
    "        if self.step_counter % self.update_rate == 0:\n",
    "            self.q_target_net.set_weights(self.q_net.get_weights())\n",
    "        #sample env data from replay buffer\n",
    "        state_batch, action_batch, reward_batch, new_state_batch, done_batch = \\\n",
    "            self.buffer.sample(self.batch_size)\n",
    "\n",
    "        self.buffer.sample(self.batch_size)\n",
    "\n",
    "        #predict q value based on main policy\n",
    "        q_predicted = self.q_net(state_batch)\n",
    "        q_target = np.copy(q_predicted)\n",
    "\n",
    "        #predict next q value based on target policy to lower correlation between states\n",
    "        q_next = self.q_target_net(new_state_batch)\n",
    "        q_max_next = tf.math.reduce_max(q_next, axis=1, keepdims=True).numpy()\n",
    "        #Bellman  Equation\n",
    "        q_target[:, action_batch]=reward_batch+self.gamma*q_max_next*(np.logical_not(done_batch)).astype(int)\n",
    "        self.q_net.train_on_batch(state_batch, q_target)\n",
    "       # self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.epsilon_final else self.epsilon_final\n",
    "        self.step_counter += 1\n",
    "    def save_network(self,txt,i,num_episodes,score,avg_score):\n",
    "        self.q_net.save((fr\"saved_networks/dqn_model{i}\"))\n",
    "        self.q_net.save_weights((fr\"saved_networks/dqn_model{i}/net_weights{i}.h5\"))\n",
    "        txt.write(fr\"Save {i} - Episode {i}/{num_episodes}, Score: {score} ({self.epsilon}), AVG Score: {avg_score}\\n\")\n",
    "        print(\"Network saved\")\n",
    "    def train_model(self, env, num_episodes, ):\n",
    "        scores, episodes, avg_scores  = [], [], []\n",
    "        txt = open(\"saved_networks.txt\", \"w\")\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            done = False\n",
    "            score = 0.0\n",
    "            state = env.reset()[0]\n",
    "            while not done and (score>-500):\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                # explore\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                # exploit\n",
    "                   # print(state)\n",
    "                    actions=self.q_net(tf.expand_dims(state,0))\n",
    "                    action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
    "                new_state, reward, done, _,_ = env.step(action)\n",
    "                #if len(new_state)>1:\n",
    "                #    new_state=new_state[0]\n",
    "                score += reward\n",
    "                reward+=float(done)*100\n",
    "                self.buffer.add(state, action, reward, new_state, done)\n",
    "                state = new_state\n",
    "                if len(self.buffer)>=3*self.batch_size:\n",
    "                    self.train_step()\n",
    "            if self.epsilon>self.epsilon_final:\n",
    "                self.epsilon-=self.epsilon_decay\n",
    "\n",
    "            scores.append(score)\n",
    "            episodes.append(i)\n",
    "            avg_score = np.mean(scores[-20:])\n",
    "            avg_scores.append(avg_score)\n",
    "            print(f\"Episode {i}/{num_episodes}, Score: {score} ({self.epsilon}), AVG Score: {avg_score}\")\n",
    "            self.save_network(txt,i,num_episodes,score,avg_score)\n",
    "        txt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def2fb22-94e2-49e3-8089-fec6bcf9dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning \n",
    "agent = Agent(alpha=1e-5, n_actions=env.action_space.n)\n",
    "\n",
    "for i in range(n_games):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    steps=0\n",
    "    while not done and steps<500:\n",
    "        steps+=1\n",
    "        if len(observation)==2:\n",
    "            continue\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info,_ = env.step(action[0])\n",
    "        score += reward\n",
    "        reward=reward+float(done)*100\n",
    "        if not load_checkpoint:\n",
    "            agent.learn(observation, reward, observation_, done)\n",
    "        observation = observation_\n",
    "    if score>best_score:\n",
    "        best_score=score\n",
    "        print(\"save_model\")\n",
    "        agent.save_model(fr\"Downloads/dqn_checkpoint/my_{score}_model.h5\")\n",
    "    if score>-120:\n",
    "        break\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-20:])\n",
    "    print(f\"Episode:{i} - score: {score} - average score: {avg_score}, steps:{steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381bf686-95a8-4d5c-8785-b44ab99aa00e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
