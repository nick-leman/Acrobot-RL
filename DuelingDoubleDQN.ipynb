{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a926894-3c6d-4fab-9ff6-85f223c4c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466a84c-928f-4e1c-9a58-fd8cdc9e7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a118c3-d546-484d-a61c-22ea35df2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5290d3-1b97-4524-b6ca-56e22bfe4a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "\n",
    "\n",
    "NUM_STATES = env.observation_space.shape[0]\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "\n",
    "print(f'States: {NUM_STATES} all ot them continuius')\n",
    "print(f'theta1 is the angle of the first joint')\n",
    "print(\"theta2 is relative to the angle of the first link\")\n",
    "print('Actions: {}'.format(NUM_ACTIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e88770-5a9b-4c1f-8400-b1435471d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 # learning rate\n",
    "gamma = 0.99 # параметр дисконтирования\n",
    "\n",
    "NUM_EPISODES = 1000 # число эпизодов для обучения\n",
    "MAX_STEPS = 500 # максимальное число шагов в эпизоде\n",
    "\n",
    "REWARD_AVERAGE_WINDOW = 20 # окно для усреднения наград по эпизодам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223ec0e-bff5-4462-b58e-a59ed35dc084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, lr, gamma, NUM_ACTIONS,NUM_STATES, batch_size, epsilon=0.95, ):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_decay = 0.001\n",
    "        self.epsilon_final = 0.05\n",
    "        self.update_rate = 100\n",
    "        self.step_counter = 0\n",
    "        self.buffer = ReplayBuffer(100000)\n",
    "        self.q_net = Q_Model(lr, NUM_ACTIONS, NUM_STATES,)\n",
    "        self.q_target_net = Q_Model(lr, NUM_ACTIONS, NUM_STATES, )\n",
    "        \n",
    "    \n",
    "    def train_step(self):\n",
    "        \n",
    "        #update target network after n steps\n",
    "        if self.step_counter % self.update_rate == 0:\n",
    "            self.q_target_net.set_weights(self.q_net.get_weights())\n",
    "        #sample env data from replay buffer\n",
    "        state_batch, action_batch, reward_batch, new_state_batch, done_batch = \\\n",
    "            self.buffer.sample(self.batch_size)\n",
    "\n",
    "        self.buffer.sample(self.batch_size)\n",
    "\n",
    "        #predict q value based on main policy\n",
    "        q_predicted = self.q_net(state_batch)\n",
    "        q_target = np.copy(q_predicted)\n",
    "\n",
    "        #predict next q value based on target policy to lower correlation between states\n",
    "        q_next = self.q_target_net(new_state_batch)\n",
    "        q_max_next = tf.math.reduce_max(q_next, axis=1, keepdims=True).numpy()\n",
    "        #Bellman  Equation\n",
    "        q_target[:, action_batch]=reward_batch+self.gamma*q_max_next*(np.logical_not(done_batch)).astype(int)\n",
    "        self.q_net.train_on_batch(state_batch, q_target)\n",
    "       # self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.epsilon_final else self.epsilon_final\n",
    "        self.step_counter += 1\n",
    "    def save_network(self,txt,i,num_episodes,score,avg_score):\n",
    "        self.q_net.save((fr\"saved_networks/dqn_model{i}\"))\n",
    "        self.q_net.save_weights((fr\"saved_networks/dqn_model{i}/net_weights{i}.h5\"))\n",
    "        txt.write(fr\"Save {i} - Episode {i}/{num_episodes}, Score: {score} ({self.epsilon}), AVG Score: {avg_score}\\n\")\n",
    "        print(\"Network saved\")\n",
    "    def train_model(self, env, num_episodes, ):\n",
    "        scores, episodes, avg_scores  = [], [], []\n",
    "        txt = open(\"saved_networks.txt\", \"w\")\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            done = False\n",
    "            score = 0.0\n",
    "            state = env.reset()[0]\n",
    "            while not done and (score>-500):\n",
    "                if np.random.rand() < self.epsilon:\n",
    "                # explore\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                # exploit\n",
    "                   # print(state)\n",
    "                    actions=self.q_net(tf.expand_dims(state,0))\n",
    "                    action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
    "                new_state, reward, done, _,_ = env.step(action)\n",
    "                #if len(new_state)>1:\n",
    "                #    new_state=new_state[0]\n",
    "                score += reward\n",
    "                reward+=float(done)*100\n",
    "                self.buffer.add(state, action, reward, new_state, done)\n",
    "                state = new_state\n",
    "                if len(self.buffer)>=3*self.batch_size:\n",
    "                    self.train_step()\n",
    "            if self.epsilon>self.epsilon_final:\n",
    "                self.epsilon-=self.epsilon_decay\n",
    "\n",
    "            scores.append(score)\n",
    "            episodes.append(i)\n",
    "            avg_score = np.mean(scores[-20:])\n",
    "            avg_scores.append(avg_score)\n",
    "            print(f\"Episode {i}/{num_episodes}, Score: {score} ({self.epsilon}), AVG Score: {avg_score}\")\n",
    "            self.save_network(txt,i,num_episodes,score,avg_score)\n",
    "        txt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd151df9-f408-4eb0-914e-813ea981e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent =Agent(lr=0.1, gamma=0.99, NUM_ACTIONS=NUM_ACTIONS,NUM_STATES=NUM_STATES, epsilon=0.95, batch_size=32)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "dqn_agent.train_model(env, num_episodes=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
